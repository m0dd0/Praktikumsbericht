% Introductions, scopr of adaptions
While all of the bencharked algorithm claimed to achieve a success rate of over 90\% in their respective settings, a number of adaptions were necessary to
make them work in our simulation environment at all and reduce the influences caused by the different setting.
In general we tried to adapt the algorithm itself as less as possible to keep the results as comparable as possible.
We never retrained the underlying networks on our data or made any othes algorithmic changed to the bencharked algorithms.
However, we had to make some adaptions to the pre- and postprocessing steps of the algorithms to make them work with the benchmarking framework described in \ref{}.

% Packaging
Another asect of the adaptions was the packaging of the benchmarked algorithms.
None of the benchmarked algorithms were implemented to work as standalone applications.
To use the implementation from within a running ROS serivce we had to refactor the code in order to package it as a standalone application.
Also the dependency management of the benchmarked algorithms needed to be adapted.

% Formulas for image <-> world transformation

\subsection{GrConvNet}
% Center cropping
% While analysing the implementation of the paer 
The preprocessing stage of the GrConvNet implementaion incorporrates a simple center cropping of the input image to the desired input size of $224x224$ pixels.
This works flawless as long as the object of interest is centered in the image and covers only an area smaller than the cropped image area.
However, when using simulation data with different camera perspectives, the object of interest is not always centered in the image.
As a workaround we replaced the center cropping mechanism with a combination of Square cropping and resizing.
This approach allows us to obtain the correct input dimension while still keeping the object of interest in the center of the image and also in reasonable size.

% segmentation
We observed that an additional previous segmentation is essential for the GrConvNet algorithm to work properly.
This comes due to the fact that the dataset the network was trained on contains white background images only.
Having a multicolored backgoudn with different color gradients in the backgroud therfore leads to the network predicting grasps in the background and not on the target object.
Addin an object segmentation layer to the preprocessing of the images resulted in a significant improvement of the grasp quality.
% which parameter set was used

% conversions to world space
The used conversion in the referenced implementation were only valid for the edge case in which we can assume that the camera is parallel to the ground plane.
To create a more general solution we first transformed the used grasp representation explained in section \ref{} to a more general representation using homogenous coordinates.
After this transformation we can use the following formulas to transform the grasp representation from image space to world space:
% TODO: add formulas

In the \cite{kumra2020antipodal} paper and also in the accompanying implementation no information were provide on how any information on how the grasp height was estimated.


\subsection{ContactGraspNet}
During development we observed significant difference in the grasp quality depending on the nuber of points being contained in the used pointcloud.

While directly offering a 6D grasp representation, there is still some conversion of the Grasp pose necessary in order to obtain a correct grasp in the simulation environement.
This mainly arises from different end effector coordinate systems used by the simulation and the algorithm.
Both representation define the z-axis as the axis along which the object is attempted in order to be grasped.
However, the simulation uses the y-axis as the axis along which the object is grasped while the algorithm uses the x-axis.
This results in the following conversion of the grasp orientation matrix beeing necessary:

\subsection{GGCNN}
