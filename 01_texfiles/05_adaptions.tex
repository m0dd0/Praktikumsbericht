% Packaging
None of the benchmarked algorithms were implemented to work as standalone applications.
To use the implementation in conjuction with the benchmarking framework described in \ref{} we had to refactor the code in order to package it as a standalone application.
Also the dependy management of the benchmarked algorithms was not optimal.
To solve

Apart from these general adaption there were necaseveral algorithm

In order to use the implementation of the benchmarked algorithms in conjuction with the benchmarking framework described in \ref{} we had to adapt
the pre- and postprocessing steps so that they fit the data obtained by the simulation.

In general we tried to adapt the algorithm as less as possible

The adaptions made to the algorithms are described in the following sections.

% Formulas for image <-> world transformation

\subsection{GrConvNet}
% Center cropping
% While analysing the implementation of the paer 
The preprocessing stage of the GrConvNet implementaion incorporrates a simple center cropping of the input image to the desired input size of $224x224$ pixels.
This works flawless as long as the object of interest is centered in the image and covers only an area smaller than the cropped image area.
However, when using simulation data with different camera perspectives, the object of interest is not always centered in the image.
As a workaround we replaced the center cropping mechanism with a combination of Square cropping and resizing.
This approach allows us to obtain the correct input dimension while still keeping the object of interest in the center of the image and also in reasonable size.

% segmentation
We observed that an additional previous segmentation is essential for the GrConvNet algorithm to work properly.
This comes due to the fact that the dataset the network was trained on contains white background images only.
Having a multicolored backgoudn with different color gradients in the backgroud therfore leads to the network predicting grasps in the background and not on the target object.
Addin an object segmentation layer to the preprocessing of the images resulted in a significant improvement of the grasp quality.
% which parameter set was used

% conversions to world space
The used conversion in the referenced implementation were only valid for the edge case in which we can assume that the camera is parallel to the ground plane.
To create a more general solution we first transformed the used grasp representation explained in section \ref{} to a more general representation using homogenous coordinates.
After this transformation we can use the following formulas to transform the grasp representation from image space to world space:
% TODO: add formulas

\subsection{ContactGraspNet}
During development we observed significant difference in the grasp quality depending on the nuber of points being contained in the used pointcloud.

While directly offering a 6D grasp representation, there is still some conversion of the Grasp pose necessary in order to obtain a correct grasp in the simulation environement.
This mainly arises from different end effector coordinate systems used by the simulation and the algorithm.
Both representation define the z-axis as the axis along which the object is attempted in order to be grasped.
However, the simulation uses the y-axis as the axis along which the object is grasped while the algorithm uses the x-axis.
This results in the following conversion of the grasp orientation matrix beeing necessary:

\subsection{GGCNN}
