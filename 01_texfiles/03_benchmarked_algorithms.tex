We selected 3 different algorithms for benchmareking.
While the benchmark focusses on deep learning based grasping algorithm the algorithms were selected so that different approaches to the task of object grasping
are represented.
Two of the benchmarked algorithms use image based inputs and return planar grasps.
The third algorithm uses pointcloud based inputs and returns 6D grasps.
This selection might allow to get an deeper understanding of the strengths and weaknesses of the different approaches and input modalities used.
A more practical aspect in the selection of the benchmarked algorithms was the availability of an publicly available source code.

% TODO reference
To allow for a better understanding of the results which we will discuss in \ref{} we will give a short description over the benchmarked
algorithms in the following subsections focussing on the algorithmic approaches.
Results and error analysis will be discussed in \ref{}.
% -- done


\subsection{GrConvNet}
% general
The GrConvNet algorithm described in \cite{kumra2020antipodal} processes an 4-channel RGBD image of the scene with a deep convolutional network to predict multiple
planar grasp candidates.
% architecture
The network architecture consists of a set of convolutional layers followed by a symmetrical set of transposed convolutional layers.
To achieve better performance the used network architecture utilizes batch normalization layers, ReLU activation functions and residual connections.
% input and output
Due to the symmetrical architecture the first dimensions of the network output correspond to the dimensions of the input image.
As a consequence the output can be interpreted as a distribution of grasp candidates in the image space.
Each grasp in the image space is represented by it's pixel coordinate, a grasp width $W$, a grasp angle $\Theta_i$ referencing the image u-axis and a grasp quality value $Q$:
$G_i = (u,v,\Theta_i, W_i, Q)$
% postprocessing
To obtain the elite grasp candidates from the grasp distribution non-maximum suppression algorithm is used to filter grasp candidates which are below a predefined
quality value or are too close to another but better grasp candidate.
In an upfollowing postprocessing stage the grasp representation of the elite grasp candidates in the image space are transformed to the world space using the
cameras extrinsic and intrinsic parameters.
% TODO: maybe visulaization here
% training
% TODO cite
The training of the network was done disjunct on the Cornell \cite{}, Dexnet \cite{} and Jacquard \cite{} dataset. To the best of our knowledge no training was executed on a combination of the datasets.
As an training objective the sum of the log-likelihood of the elite grasp candidates being close to the ground gruth grasp candidates was used.
For a training dataset with $n$ images and $m$ grasp candidates this objective can be expressed as:
$L = \sum_{i=1}^n \sum_{j=1}^m \log p(G_j|I_i)$
where $p(G_j|I_i)$ is the probability of the $j$th grasp candidate in the $i$th image being similar enough to the ground truth grasp candidate.
The simmilarity of a predicted grasp to the ground truth grasp is determined by the intersection over union of the grap rectangles and the differenc in their grasp angles.
Due to empirical analysis of the performance Huber Loss was selected as loss function.
% -- done


\subsection{ContactGraspNet}
% general
In \cite{sundermeyer2021contact} the point cloud based \textit{ContactGraspNet} algorithm is presented which can be viewed as a successor of the method described in \cite{}.
The algorithm is based on the idea that in a partially observable environment, the search space for grasp candidates can be limited to the set of grasps which are
in contact with one of the points in the observed pointcloud.
% network architecture
For each point in the give pointcloud the network predics whether this point is suitable as a contact point for grasping, the width of a grasp and the the approach vector a and the grasp axis b.
This information is used to generate a set of 6D grasp representation.
% network architecture
The presented network architecture is a adaption of the Pointnet++ architecture \cite{qi2017pointnet++} which is used as a feauture extractor for pointcloud.
To reduce computational complexity the network predicts.
Note that this reduction only refers to the decoding part of the network. The network still processes the full pointcloud in the encoding layers in order to
utilize the spatial strucures consisting of neighboring points.
%  TODO
% input and output
In contrast to \textit{GrConvNet} and \textit{GGCNN} the \textit{ContactGraspNet} is not limited to grasps parallel to a pedefined surface.
Instead the algorithm can be used to gather grasps with an arbitrary orientation.
Consequently a grasp is represented by a 6D pose consisting of a position and an orientation:
$$ G = (R_g, t_g) = $$
Each observed point can be used as an anchor point for a grasp.
A visulization of this grasp representation in our simulation environment is shown in \ref{}.
The network architecture described uses the full pointcloud to obtain a number of grasp candidates for all objects present in the pointcloud.
However, the presented implementation in \ref{} also adds the ability to filter the grasp candidate based on a provided segmentation.


% training
Due to it's pointcloud and the assumption that we can only a single depth image to capture a partial pointcloud of the environment based anatomy the traini
Instead they utilized ACRONYM dataset and the Shapenet dataset
The training procedure can be summarized as follows:
\begin{enumerate}
\item Generating a simulated scene with objects (and annotated grasps) of from the ACRONYM dataset in random stable poses.
\item Filter grasps which collide with oder objects in the scene
\item Create a pointcloud from a single depth image of the scene
\item The network is trained on the Shapenet dataset to predict the grasp pose.
% The grasp axis can be oriented in arbitrary direction


\subsection{GGCNN}
Siimilarly to the \textit{GrConvNet} algorithm the \textit{GGCNN} algorithm based on image inputs and returns image based grasp representations.
\subsection{Comparison}
